[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "",
    "text": "Fit a polynomial of order 8 using least squares, without using R functions such as lm(). Plot the training data, test data and the fitted polynomial in the same plot.\n\nThe following code obtains the data, then uses the logarithm of the amount of times that people used bikes and divides by 23 the hours when they take the bikes to normalize the hours between the [0,1] interval:\n\nrm(list=ls()) # Remove variables \ncat(\"\\014\") # Clean workspace\n\nbike_data &lt;- read.csv('/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 1/bike_rental_hourly.csv')\nbike_data$log_cnt &lt;- log(bike_data$cnt)\nbike_data$hour &lt;- bike_data$hr/23\n\nThe following code estimates the model (a polynomial of order 8) using least squares. Moreover, the code plots the training data, test data and the fitted polynomial in the same plot:\n\nbike_data_train &lt;- bike_data[bike_data$dteday &gt;= as.Date(\"2011-02-01\") & bike_data$dteday &lt;=  as.Date(\"2011-03-31\"), ]\nbike_data_test &lt;- bike_data[bike_data$dteday &gt;= as.Date(\"2011-04-01\") & bike_data$dteday &lt;=  as.Date(\"2011-05-31\"), ]\ny_train &lt;- bike_data_train$log_cnt\ny_test &lt;- bike_data_test$log_cnt\np &lt;- 8 # Order of polynomial\n\n# Design matrix / matrix of features (including intercept), the function generates raw (not orthogonal) polynomial terms and is returned as a simple matrix\nX_train &lt;- cbind(1, poly(bike_data_train$hour, p, raw = TRUE, simple = TRUE))\n\n# Obtaining beta_hat using the OLS formula\nbeta_hat &lt;- solve(t(X_train)%*%X_train)%*%t(X_train)%*%y_train\n\n# Predict in-sample\ny_hat_train &lt;- X_train%*%beta_hat\n\n# Design matrix / matrix of features (including intercept)\nX_test &lt;- cbind(1, poly(bike_data_test$hour, p, raw = TRUE, simple = TRUE))\n\n# Predict out-of-sample\ny_hat_test &lt;- X_test%*%beta_hat\n\n# Plot training data, test data, and fit on a fine grid.\nplot(log_cnt ~ hour, data = bike_data_train, col = \"cornflowerblue\", ylim = c(0, 8),main = \"Training data, test data, and fitted polynomial of 8-th degree\", cex.main = 0.75, xlab = \"Hours\", ylab = \"Log_cnt\")\nlines(bike_data_test$hour, bike_data_test$log_cnt, type = \"p\", col = \"lightcoral\")\nhours_grid &lt;- seq(0, 1, length.out = 1000)\nX_grid &lt;- cbind(1, poly(hours_grid, p, raw = TRUE, simple = TRUE))\ny_hat_grid &lt;- X_grid %*% beta_hat\nlines(hours_grid, y_hat_grid, lty = 1, col = \"lightcoral\")\nlegend(x = \"topleft\", pch = c(1, 1, NA), lty = c(NA, NA, 1), col = c(\"cornflowerblue\", \"lightcoral\", \"lightcoral\"), legend = c(\"Train\", \"Test\", \"Fitted curve\"), cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\nFit polynomials of varying order 1-10 using least squares, without using R functions such as lm(). Compute the RMSEs for the training and test data and plot them on the same figure as a function of the polynomial order. Are you underfitting or overfitting the data?\n\nThe following code is a loop that changes the order of the polynomial from 1 to 10 and computes and stores the RMSE for each polynomial degree:\n\n# Vectors to store RMSEs\nrmse_train &lt;- numeric(10)\nrmse_test &lt;- numeric(10)\n\n# Loop over polynomial degrees from 1 to 10\nfor (p in 1:10) \n  {# Design matrix / matrix of features (including intercept)\n  X_train_loop &lt;- cbind(1, poly(bike_data_train$hour, p, raw = TRUE, simple = TRUE))\n  \n  # Calculate beta_hat using the OLS formula\n  beta_hat_loop &lt;- solve(t(X_train_loop) %*% X_train_loop) %*% t(X_train_loop) %*% y_train\n  \n  # Predict in-sample\n  y_hat_train_loop &lt;- X_train_loop %*% beta_hat_loop\n  \n  # Calculate RMSE for training data\n  rmse_train[p] &lt;- sqrt(sum((y_train - y_hat_train_loop)^2) / length(y_train))\n  \n  # Design matrix / matrix of features (including intercept)\n  X_test_loop &lt;- cbind(1, poly(bike_data_test$hour, p, raw = TRUE, simple = TRUE))\n  \n  # Predict out-of-sample\n  y_hat_test_loop &lt;- X_test_loop %*% beta_hat_loop\n  \n  # Calculate RMSE for test data\n  rmse_test[p] &lt;- sqrt(sum((y_test - y_hat_test_loop)^2) / length(y_test))}\n\n# Print the first six RMSE's for each polynomial degree\nhead(data.frame(Polynomial_Degree = 1:10, RMSE_Train = rmse_train, RMSE_Test = rmse_test))\n\n  Polynomial_Degree RMSE_Train RMSE_Test\n1                 1  1.1784759  1.323750\n2                 2  0.9872309  1.208817\n3                 3  0.9225993  1.149316\n4                 4  0.8675218  1.116446\n5                 5  0.7975209  1.069621\n6                 6  0.7925395  1.065390\n\n\nNow letâ€™s plot it!\n\n# Plot \nplot(1:10, rmse_train, type = \"b\", col = \"cornflowerblue\", pch = 1, lty = 5, ylim = range(c(rmse_train, rmse_test)),xlab = \"Polynomial Degree\", ylab = \"RMSE\", main = \"RMSE vs. Polynomial Degree\", cex.main = 0.75)\n\n# Add the test RMSE to the plot\nlines(1:10, rmse_test, type = \"b\", col = \"lightcoral\", pch = 1, lty = 5)\n\n# Legend\nlegend(\"topright\", legend = c(\"Train\", \"Test\"), col = c(\"cornflowerblue\", \"lightcoral\"), pch = 1, lty = 5)\n\n\n\n\n\n\n\n\nWhen looking at the RMSE chart created above, as the polynomial degree increases, the training error decreases steadily, indicating that the model is fitting the training data well.\n\n\n\n\nPolynomials are global functions and their fit may be sensitive to outliers. Local fitting methods can sometimes be more robust. One such method is the loess method, which is a nonparametric method that fits locally weighted regressions to subsets of points and subsequently combines them to obtain a global fit. Use the loess function in R with the standard settings to fit a locally weighted regression to the training data. Is this method better than that in Problem 1.1? Plot the training data, test data and both fitted curves in the same plot.\n\nThe following code fits a locally weighted regression to the data and plots the training data, test data and both fitted curves:\n\n# Fit a LOESS model to the training data\nloess_model &lt;- loess(log_cnt ~ hour, data = bike_data_train)\n\n# Predict using LOESS\ny_hat_train_loess &lt;- predict(loess_model, newdata = bike_data_train$hour) \ny_hat_test_loess &lt;- predict(loess_model, newdata = bike_data_test$hour)\ny_hat_grid_loess &lt;- predict(loess_model, newdata = data.frame(hour = hours_grid))\n\n# Plot\nplot(log_cnt ~ hour, data = bike_data_train, col = \"cornflowerblue\", ylim = c(0, 8), main = \"Training data, test data, and fitted curves (Polynomial & LOESS)\", \n     cex.main = 0.75, xlab = \"Hours\", ylab = \"Log_cnt\")\nlines(bike_data_test$hour, bike_data_test$log_cnt, type = \"p\", col = \"lightcoral\")\n\n# Polynomial fit\nlines(hours_grid, y_hat_grid, lty = 1, col = \"lightcoral\")\n\n# LOESS fit\nlines(hours_grid, y_hat_grid_loess, lty = 2, col = \"darkgreen\")\n\n# Legend\nlegend(x = \"topleft\", pch = c(1, 1, NA, NA), lty = c(NA, NA, 1, 2), \n       col = c(\"cornflowerblue\", \"lightcoral\", \"lightcoral\", \"darkgreen\"), \n       legend = c(\"Train\", \"Test\", \"Polynomial Fit\", \"LOESS Fit\"), cex = 0.6)\n\n\n\n\n\n\n\n\nWe can calculate the RMSE for each method to know which one performs better in the test data:\n\n# Calculate RMSE for test data using least squares\nrmse_test_poly &lt;- sqrt(mean((y_test - y_hat_test)^2))\nprint(paste(\"The RMSE for the test data using polynomial of order 8 and least squares:\", rmse_test_poly))\n\n[1] \"The RMSE for the test data using polynomial of order 8 and least squares: 1.02144928808121\"\n\n# Calculate RMSE for test data using LOESS\nrmse_test_LOESS &lt;- sqrt(mean((y_test - y_hat_test_loess)^2))\nprint(paste(\"The RMSE for the test data using LOESS:\", rmse_test_LOESS))\n\n[1] \"The RMSE for the test data using LOESS: 1.12094586786539\"\n\n\nThe polynomial of order 8 has a lower RMSE in the test data than LOESS, but as seen in the chart, it overfits the data."
  },
  {
    "objectID": "index.html#polynomial-regression-for-bike-rental-data",
    "href": "index.html#polynomial-regression-for-bike-rental-data",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "",
    "text": "Fit a polynomial of order 8 using least squares, without using R functions such as lm(). Plot the training data, test data and the fitted polynomial in the same plot.\n\nThe following code obtains the data, then uses the logarithm of the amount of times that people used bikes and divides by 23 the hours when they take the bikes to normalize the hours between the [0,1] interval:\n\nrm(list=ls()) # Remove variables \ncat(\"\\014\") # Clean workspace\n\nbike_data &lt;- read.csv('/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 1/bike_rental_hourly.csv')\nbike_data$log_cnt &lt;- log(bike_data$cnt)\nbike_data$hour &lt;- bike_data$hr/23\n\nThe following code estimates the model (a polynomial of order 8) using least squares. Moreover, the code plots the training data, test data and the fitted polynomial in the same plot:\n\nbike_data_train &lt;- bike_data[bike_data$dteday &gt;= as.Date(\"2011-02-01\") & bike_data$dteday &lt;=  as.Date(\"2011-03-31\"), ]\nbike_data_test &lt;- bike_data[bike_data$dteday &gt;= as.Date(\"2011-04-01\") & bike_data$dteday &lt;=  as.Date(\"2011-05-31\"), ]\ny_train &lt;- bike_data_train$log_cnt\ny_test &lt;- bike_data_test$log_cnt\np &lt;- 8 # Order of polynomial\n\n# Design matrix / matrix of features (including intercept), the function generates raw (not orthogonal) polynomial terms and is returned as a simple matrix\nX_train &lt;- cbind(1, poly(bike_data_train$hour, p, raw = TRUE, simple = TRUE))\n\n# Obtaining beta_hat using the OLS formula\nbeta_hat &lt;- solve(t(X_train)%*%X_train)%*%t(X_train)%*%y_train\n\n# Predict in-sample\ny_hat_train &lt;- X_train%*%beta_hat\n\n# Design matrix / matrix of features (including intercept)\nX_test &lt;- cbind(1, poly(bike_data_test$hour, p, raw = TRUE, simple = TRUE))\n\n# Predict out-of-sample\ny_hat_test &lt;- X_test%*%beta_hat\n\n# Plot training data, test data, and fit on a fine grid.\nplot(log_cnt ~ hour, data = bike_data_train, col = \"cornflowerblue\", ylim = c(0, 8),main = \"Training data, test data, and fitted polynomial of 8-th degree\", cex.main = 0.75, xlab = \"Hours\", ylab = \"Log_cnt\")\nlines(bike_data_test$hour, bike_data_test$log_cnt, type = \"p\", col = \"lightcoral\")\nhours_grid &lt;- seq(0, 1, length.out = 1000)\nX_grid &lt;- cbind(1, poly(hours_grid, p, raw = TRUE, simple = TRUE))\ny_hat_grid &lt;- X_grid %*% beta_hat\nlines(hours_grid, y_hat_grid, lty = 1, col = \"lightcoral\")\nlegend(x = \"topleft\", pch = c(1, 1, NA), lty = c(NA, NA, 1), col = c(\"cornflowerblue\", \"lightcoral\", \"lightcoral\"), legend = c(\"Train\", \"Test\", \"Fitted curve\"), cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\nFit polynomials of varying order 1-10 using least squares, without using R functions such as lm(). Compute the RMSEs for the training and test data and plot them on the same figure as a function of the polynomial order. Are you underfitting or overfitting the data?\n\nThe following code is a loop that changes the order of the polynomial from 1 to 10 and computes and stores the RMSE for each polynomial degree:\n\n# Vectors to store RMSEs\nrmse_train &lt;- numeric(10)\nrmse_test &lt;- numeric(10)\n\n# Loop over polynomial degrees from 1 to 10\nfor (p in 1:10) \n  {# Design matrix / matrix of features (including intercept)\n  X_train_loop &lt;- cbind(1, poly(bike_data_train$hour, p, raw = TRUE, simple = TRUE))\n  \n  # Calculate beta_hat using the OLS formula\n  beta_hat_loop &lt;- solve(t(X_train_loop) %*% X_train_loop) %*% t(X_train_loop) %*% y_train\n  \n  # Predict in-sample\n  y_hat_train_loop &lt;- X_train_loop %*% beta_hat_loop\n  \n  # Calculate RMSE for training data\n  rmse_train[p] &lt;- sqrt(sum((y_train - y_hat_train_loop)^2) / length(y_train))\n  \n  # Design matrix / matrix of features (including intercept)\n  X_test_loop &lt;- cbind(1, poly(bike_data_test$hour, p, raw = TRUE, simple = TRUE))\n  \n  # Predict out-of-sample\n  y_hat_test_loop &lt;- X_test_loop %*% beta_hat_loop\n  \n  # Calculate RMSE for test data\n  rmse_test[p] &lt;- sqrt(sum((y_test - y_hat_test_loop)^2) / length(y_test))}\n\n# Print the first six RMSE's for each polynomial degree\nhead(data.frame(Polynomial_Degree = 1:10, RMSE_Train = rmse_train, RMSE_Test = rmse_test))\n\n  Polynomial_Degree RMSE_Train RMSE_Test\n1                 1  1.1784759  1.323750\n2                 2  0.9872309  1.208817\n3                 3  0.9225993  1.149316\n4                 4  0.8675218  1.116446\n5                 5  0.7975209  1.069621\n6                 6  0.7925395  1.065390\n\n\nNow letâ€™s plot it!\n\n# Plot \nplot(1:10, rmse_train, type = \"b\", col = \"cornflowerblue\", pch = 1, lty = 5, ylim = range(c(rmse_train, rmse_test)),xlab = \"Polynomial Degree\", ylab = \"RMSE\", main = \"RMSE vs. Polynomial Degree\", cex.main = 0.75)\n\n# Add the test RMSE to the plot\nlines(1:10, rmse_test, type = \"b\", col = \"lightcoral\", pch = 1, lty = 5)\n\n# Legend\nlegend(\"topright\", legend = c(\"Train\", \"Test\"), col = c(\"cornflowerblue\", \"lightcoral\"), pch = 1, lty = 5)\n\n\n\n\n\n\n\n\nWhen looking at the RMSE chart created above, as the polynomial degree increases, the training error decreases steadily, indicating that the model is fitting the training data well.\n\n\n\n\nPolynomials are global functions and their fit may be sensitive to outliers. Local fitting methods can sometimes be more robust. One such method is the loess method, which is a nonparametric method that fits locally weighted regressions to subsets of points and subsequently combines them to obtain a global fit. Use the loess function in R with the standard settings to fit a locally weighted regression to the training data. Is this method better than that in Problem 1.1? Plot the training data, test data and both fitted curves in the same plot.\n\nThe following code fits a locally weighted regression to the data and plots the training data, test data and both fitted curves:\n\n# Fit a LOESS model to the training data\nloess_model &lt;- loess(log_cnt ~ hour, data = bike_data_train)\n\n# Predict using LOESS\ny_hat_train_loess &lt;- predict(loess_model, newdata = bike_data_train$hour) \ny_hat_test_loess &lt;- predict(loess_model, newdata = bike_data_test$hour)\ny_hat_grid_loess &lt;- predict(loess_model, newdata = data.frame(hour = hours_grid))\n\n# Plot\nplot(log_cnt ~ hour, data = bike_data_train, col = \"cornflowerblue\", ylim = c(0, 8), main = \"Training data, test data, and fitted curves (Polynomial & LOESS)\", \n     cex.main = 0.75, xlab = \"Hours\", ylab = \"Log_cnt\")\nlines(bike_data_test$hour, bike_data_test$log_cnt, type = \"p\", col = \"lightcoral\")\n\n# Polynomial fit\nlines(hours_grid, y_hat_grid, lty = 1, col = \"lightcoral\")\n\n# LOESS fit\nlines(hours_grid, y_hat_grid_loess, lty = 2, col = \"darkgreen\")\n\n# Legend\nlegend(x = \"topleft\", pch = c(1, 1, NA, NA), lty = c(NA, NA, 1, 2), \n       col = c(\"cornflowerblue\", \"lightcoral\", \"lightcoral\", \"darkgreen\"), \n       legend = c(\"Train\", \"Test\", \"Polynomial Fit\", \"LOESS Fit\"), cex = 0.6)\n\n\n\n\n\n\n\n\nWe can calculate the RMSE for each method to know which one performs better in the test data:\n\n# Calculate RMSE for test data using least squares\nrmse_test_poly &lt;- sqrt(mean((y_test - y_hat_test)^2))\nprint(paste(\"The RMSE for the test data using polynomial of order 8 and least squares:\", rmse_test_poly))\n\n[1] \"The RMSE for the test data using polynomial of order 8 and least squares: 1.02144928808121\"\n\n# Calculate RMSE for test data using LOESS\nrmse_test_LOESS &lt;- sqrt(mean((y_test - y_hat_test_loess)^2))\nprint(paste(\"The RMSE for the test data using LOESS:\", rmse_test_LOESS))\n\n[1] \"The RMSE for the test data using LOESS: 1.12094586786539\"\n\n\nThe polynomial of order 8 has a lower RMSE in the test data than LOESS, but as seen in the chart, it overfits the data."
  },
  {
    "objectID": "index.html#regularised-spline-regression-for-bike-rental-data",
    "href": "index.html#regularised-spline-regression-for-bike-rental-data",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "2. Regularised spline regression for bike rental data",
    "text": "2. Regularised spline regression for bike rental data\n\nðŸ‘¾ Problem 2.1\n\nFit a spline regression to the training data with an L2 regularisation using a suitable value of \\(\\lambda\\), without using R functions such as glmnet(). Plot the fit together with the training and test data.\n\nThe following code fits a spline regression to the training data with a Ridge regularisation using a suitable value of \\(\\lambda\\) that minimizes the RMSE on the test data. Moreover, the code plots the fit together with the training and test data:\n\n# Library\nsuppressMessages(library(splines))\n\n# Equally spaced knots\nknots &lt;- seq(0.05, 0.95, length.out = 25)\n\n# Design matrices / matrices of features (including intercept)\nX_train &lt;- ns(bike_data_train$hour, knots = knots, intercept = TRUE)\nX_test &lt;- ns(bike_data_test$hour, knots = knots, intercept = TRUE)\nX_grid &lt;- ns(hours_grid, knots = knots, intercept = TRUE)\nbeta_hat &lt;- solve(t(X_train)%*%X_train)%*%t(X_train)%*%y_train\n\n# y_hat grid\ny_hat_spline_grid &lt;- X_grid%*%beta_hat\n\n# Lambda grid\nlambda_grid &lt;- seq(0, 1, length.out=100)\n\n# To store RMSE and beta values\nrmse_list &lt;- numeric(length(lambda_grid))\nbeta_list &lt;- list()\n\n# Ridge Regression for each lambda\nfor (i in seq_along(lambda_grid)) \n  {lambda &lt;- lambda_grid[i]\n  \n  # Ridge estimator\n  Ridge_betas &lt;- solve(t(X_train) %*% X_train + lambda * diag(ncol(X_train))) %*%                  t(X_train) %*% y_train\n  \n  # Store the betas\n  beta_list[[i]] &lt;- Ridge_betas\n  \n  # Predictions on test data\n  y_pred &lt;- X_test %*% Ridge_betas\n  \n  # RMSE on test data\n  rmse_list[i] &lt;- sqrt(mean((y_pred - y_test)^2))}\n\n# Optimal lambda\noptimal_lambda &lt;- lambda_grid[which.min(rmse_list)]\nprint(paste(\"The optimal lambda is:\", optimal_lambda))\n\n[1] \"The optimal lambda is: 0.0101010101010101\"\n\noptimal_beta &lt;- beta_list[[which.min(rmse_list)]]\n\n# Plot the training data, test data, and spline fits\nplot(log_cnt ~ hour, data = bike_data_train, col = \"cornflowerblue\", ylim = c(0, 8), main = \"Training Data, Test Data, and Spline Fits (Original & Regularized)\", cex.main = 0.75, xlab = \"Hours\", ylab = \"Log_cnt\")\nlines(bike_data_test$hour, bike_data_test$log_cnt, type = \"p\", col = \"lightcoral\")\n\n# Original spline fit\nlines(hours_grid, y_hat_spline_grid, lty = 1, col = \"lightcoral\")\n\n# Regularized spline fit with optimal lambda\ny_hat_spline_grid_reg &lt;- X_grid %*% optimal_beta\nlines(hours_grid, y_hat_spline_grid_reg, lty = 2, col = \"darkgreen\")\n\n# Legend\nlegend(x = \"topleft\", pch = c(1, 1, NA, NA), lty = c(NA, NA, 1, 2), \n       col = c(\"cornflowerblue\", \"lightcoral\", \"lightcoral\", \"darkgreen\"),legend = c(\"Train\", \"Test\", \"Original Spline Fit\", \"Regularized Spline Fit\"), cex = 0.6)\n\n\n\n\n\n\n\n\n\n\nðŸ‘¾ Problem 2.2\n\nUse the package glmnet to fit a spline regression with an L2 regularisation using the same basis functions as the previous problem. Find the optimal \\(\\lambda\\) by applying the one-standard deviation rule when cross-validating the training data using 10 folds. Compute the RMSE (using the optimal \\(\\lambda\\)) for the training and test data. Plot the fit together with the training and test data.\n\nThe following code fits a spline regression with a Ridge regularisation using glmnet. It finds the optimal \\(\\lambda\\) by applying the one-standard deviation rule when cross-validating the training data using 10 folds. Moreover, the code computes the RMSE (using the optimal \\(\\lambda\\)) for the training and test data and plots the fit together with the training and test data:\n\nsuppressMessages(library(glmnet)) # Library\nset.seed(123)  # For reproducibility\n# Using glmnet, for Ridge regression (Ridge, alpha = 0), the dataset is randomly divided into 10 equal parts\ncv_fit &lt;- cv.glmnet(X_train, y_train, alpha = 0, nfolds = 10)\n\n# Find the optimal lambda using the one-standard deviation rule\noptimal_lambda_new &lt;- cv_fit$lambda.1se\n\n# Predict\ny_train_pred &lt;- predict(cv_fit, newx = X_train, s = optimal_lambda_new)\ny_test_pred &lt;- predict(cv_fit, newx = X_test, s = optimal_lambda_new)\n\n# Store RMSE's\nrmse_train &lt;- sqrt(mean((y_train_pred - y_train)^2))\nrmse_test &lt;- sqrt(mean((y_test_pred - y_test)^2))\n\n# Print lambda and RMSE's values\nprint(paste(\"The optimal lambda using glmnet applying the one-standard deviation rule when cross-validating is:\", optimal_lambda_new))\n\n[1] \"The optimal lambda using glmnet applying the one-standard deviation rule when cross-validating is: 0.437818786918387\"\n\nprint(paste(\"The RMSE on Training Data is:\", rmse_train))\n\n[1] \"The RMSE on Training Data is: 0.711196193092732\"\n\nprint(paste(\"The RMSE on Test Data is:\", rmse_test))\n\n[1] \"The RMSE on Test Data is: 1.00077331573551\"\n\n# Plot\nplot(log_cnt ~ hour, data = bike_data_train, col = \"cornflowerblue\", ylim = c(0, 8), main = \"Spline Regression with L2 Regularisation (GLMNET)\", \n     cex.main = 0.75, xlab = \"Hours\", ylab = \"Log_cnt\")\nlines(bike_data_test$hour, bike_data_test$log_cnt, type = \"p\", col = \"lightcoral\")\n\n# Predict on grid using optimal lambda\ny_grid_pred &lt;- predict(cv_fit, newx = X_grid, s = optimal_lambda_new)\n\n# Spline fit line\nlines(hours_grid, y_grid_pred, lty = 1, col = \"darkgreen\")\n\n# Legend\nlegend(x = \"topleft\", pch = c(1, 1, NA), lty = c(NA, NA, 1), \n       col = c(\"cornflowerblue\", \"lightcoral\", \"darkgreen\"), \n       legend = c(\"Train\", \"Test\", \"Regularized Spline Fit\"), cex = 0.6)\n\n\n\n\n\n\n\n\n\n\nðŸ‘¾ Problem 2.3\n\nRepeat Problem 2.2, however, use the optimal \\(\\lambda\\) that minimises the mean cross-validated error. Compare the RMSE for the training and test data to those in Problem 2.2.\n\nThe following code fits a spline regression with a Ridge regularisation using glmnet. It finds the optimal \\(\\lambda\\) that minimises the mean cross-validated error and computes the RMSE for the training and test data of problem 2.3 in order to compare against problem 2.2:\n\n# Ridge regression using cross-validation (Ridge, alpha = 0)\ncv_fit &lt;- cv.glmnet(X_train, y_train, alpha = 0, nfolds = 10)\n\n# Find the optimal lambda that minimizes the mean cross-validated error\noptimal_lambda_min &lt;- cv_fit$lambda.min\n\n# Predict\ny_train_pred_min &lt;- predict(cv_fit, newx = X_train, s = optimal_lambda_min)\ny_test_pred_min &lt;- predict(cv_fit, newx = X_test, s = optimal_lambda_min)\n\n# Calculate RMSE's\nrmse_train_min &lt;- sqrt(mean((y_train_pred_min - y_train)^2))\nrmse_test_min &lt;- sqrt(mean((y_test_pred_min - y_test)^2))\n\n# Print RMSE values for lambda.min\nprint(paste(\"The RMSE on Training Data (lambda.min - Problem 2.3):\", rmse_train_min))\n\n[1] \"The RMSE on Training Data (lambda.min - Problem 2.3): 0.691963883875149\"\n\nprint(paste(\"The RMSE on Test Data (lambda.min - Problem 2.3):\", rmse_test_min))\n\n[1] \"The RMSE on Test Data (lambda.min - Problem 2.3): 1.0026681851555\"\n\n# Compare with the previous RMSE values from lambda.1se\nprint(paste(\"The RMSE on Training Data (lambda.1se - Problem 2.2):\", rmse_train))\n\n[1] \"The RMSE on Training Data (lambda.1se - Problem 2.2): 0.711196193092732\"\n\nprint(paste(\"The RMSE on Test Data (lambda.1se - Problem 2.2):\", rmse_test))\n\n[1] \"The RMSE on Test Data (lambda.1se - Problem 2.2): 1.00077331573551\"\n\n# Plot the results using lambda.min\nplot(log_cnt ~ hour, data = bike_data_train, col = \"cornflowerblue\", ylim = c(0, 8), \n     main = \"Spline Regression with L2 Regularisation (GLMNET)\", \n     cex.main = 0.75, xlab = \"Hours\", ylab = \"Log_cnt\")\nlines(bike_data_test$hour, bike_data_test$log_cnt, type = \"p\", col = \"lightcoral\")\n\n# Predict on grid using optimal lambda.min\ny_grid_pred_min &lt;- predict(cv_fit, newx = X_grid, s = optimal_lambda_min)\n\n# Add spline fit line using lambda.min\nlines(hours_grid, y_grid_pred_min, lty = 1, col = \"darkgreen\")\n\n# Add a legend\nlegend(x = \"topleft\", pch = c(1, 1, NA), lty = c(NA, NA, 1), \n       col = c(\"cornflowerblue\", \"lightcoral\", \"darkgreen\"), \n       legend = c(\"Train\", \"Test\", \"Regularized Spline Fit\"), cex = 0.6)\n\n\n\n\n\n\n\n\nThe RMSE is lower in-sample when using the optimal \\(\\lambda\\) that minimises the mean cross-validated error in comparison to the \\(\\lambda\\) that is obtained using the one-standard deviation rule. The RMSE is higher out of sample when using the optimal \\(\\lambda\\) that minimises the mean cross-validated error in comparison to the \\(\\lambda\\) that minimises the mean cross-validated error. This happens because the one-standard-deviation rule tends to favor a model with better generalization performance at the cost of a slightly higher training error, whereas the \\(\\lambda\\) that minimizes the mean cross-validated error might give better in-sample performance but can overfit and perform worse on out-of-sample data.\n\n\nðŸ‘¾ Problem 2.4\n\nRepeat Problem 2.2 using L1 regularisation. Compare the RMSE for the training and test data to those in Problem 2.2.\n\nThe following code fits a spline regression with a Lasso regularisation using glmnet. It finds the optimal \\(\\lambda\\) by applying the one-standard deviation rule when cross-validating the training data using 10 folds. Moreover, the code computes the RMSE (using the optimal \\(\\lambda\\)) for the training and test data in order to compare against problem 2.2, and plots the fit together with the training and test data:\n\n# Fit the model with 10-fold cross-validation (Lasso, alpha = 1)\ncv_fit_lasso &lt;- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)\n\n# One-standard deviation rule to select lambda\noptimal_lambda_lasso &lt;- cv_fit_lasso$lambda.1se\n\n# Predict\ny_train_pred_lasso &lt;- predict(cv_fit_lasso, newx = X_train, s = optimal_lambda_lasso)\ny_test_pred_lasso &lt;- predict(cv_fit_lasso, newx = X_test, s = optimal_lambda_lasso)\n\n# Calculate RMSE's for training and test data\nrmse_train_lasso &lt;- sqrt(mean((y_train_pred_lasso - y_train)^2))\nrmse_test_lasso &lt;- sqrt(mean((y_test_pred_lasso - y_test)^2))\n\n# Print RMSE's values for Lasso\nprint(paste(\"The RMSE on Train Data using LASSO (Problem 2.4):\", rmse_train_lasso))\n\n[1] \"The RMSE on Train Data using LASSO (Problem 2.4): 0.703807584356601\"\n\nprint(paste(\"The RMSE on Test Data using LASSO (Problem 2.4):\", rmse_test_lasso))\n\n[1] \"The RMSE on Test Data using LASSO (Problem 2.4): 1.00283700697307\"\n\n# Compare with Ridge RMSE's values\nprint(paste(\"The RMSE on Train Data using RIDGE (Problem 2.2):\", rmse_train))\n\n[1] \"The RMSE on Train Data using RIDGE (Problem 2.2): 0.711196193092732\"\n\nprint(paste(\"The RMSE on Test Data using RIDGE (Problem 2.2):\", rmse_test))\n\n[1] \"The RMSE on Test Data using RIDGE (Problem 2.2): 1.00077331573551\"\n\n# Plot \nplot(log_cnt ~ hour, data = bike_data_train, col = \"cornflowerblue\", ylim = c(0, 8), main = \"Spline Regression with L1 Regularisation (GLMNET)\", \n     cex.main = 0.75, xlab = \"Hours\", ylab = \"Log_cnt\")\nlines(bike_data_test$hour, bike_data_test$log_cnt, type = \"p\", col = \"lightcoral\")\n\n# Curve for the optimal lambda using Lasso\ny_hat_spline_grid_lasso &lt;- predict(cv_fit_lasso, newx = X_grid, s = optimal_lambda_lasso)\nlines(hours_grid, y_hat_spline_grid_lasso, lty = 1, col = \"darkgreen\")\n\n# Legend\nlegend(x = \"topleft\", pch = c(1, 1, NA), lty = c(NA, NA, 1), \n       col = c(\"cornflowerblue\", \"lightcoral\", \"darkgreen\"), \n       legend = c(\"Train\", \"Test\", \"Regularized Spline Fit\"), \n       cex = 0.6)\n\n\n\n\n\n\n\n\nThe RMSE is lower in-sample when using Lasso in comparison to when we use Ridge. The RMSE is lower out-sample when using Ridge in comparison to when we use Lasso. This happens because of Lassoâ€™s feature selection that can lead to an overly simplified model if the excluded variables are indeed relevant, which harm its out-of-sample performance in comparison to Ridgeâ€™s approach of shrinking all coefficients, avoiding this pitfall, ensuring that the model retains all relevant information, which results in better performance on out-sample data."
  },
  {
    "objectID": "index.html#regularised-regression-for-bike-rental-data-with-more-features-and-data",
    "href": "index.html#regularised-regression-for-bike-rental-data-with-more-features-and-data",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "3. Regularised regression for bike rental data with more features and data",
    "text": "3. Regularised regression for bike rental data with more features and data\n\nðŸ‘¾ Problem 3.1\n\nFit a spline regression to the training data with an L1 regularisation. Find the optimal \\(\\lambda\\) by applying the one-standard deviation rule when cross-validating the training data using 10 folds. Compute the RMSE (using the optimal \\(\\lambda\\)) for the training and test data. For the spline, use the splines package in R to create natural cubic splines basis functions of the variable hour with 10 degrees of freedom, i.e.Â use the ns() function with df=10 as input argument. Moreover, set intercept=FALSE and add it manually to your design matrix instead.\nThe following variables should be included in the regression:\n\nResponse variable: log_cnt.\nFeatures: hour (via the cubic spline described above), yr, holiday, workingday, temp, atemp, hum, windspeed, and all the dummy variables.\n\n\nThe following code creates the dummy variables, then it uses the new period of time to split the data between train and test data. Furthermore, the code uses the splines package in R to create natural cubic splines basis functions of the variable hour with 10 degrees of freedom, for the train and test data, setting the argument intercept=FALSE, adding manually the intercept column to the design matrix and using the same number of knots for both datasets. Finally the model is penalized using a Lasso regularisation. Moreover, the code finds the optimal \\(\\lambda\\) by applying the one-standard deviation rule when cross-validating the training data and prints the RMSE of the out-sample and in-sample datasets:\n\n# One hot for weathersit\none_hot_encode_weathersit &lt;- model.matrix(~ as.factor(weathersit) - 1,data = bike_data)\none_hot_encode_weathersit  &lt;- one_hot_encode_weathersit[, -1] # Remove reference category\ncolnames(one_hot_encode_weathersit) &lt;- c('cloudy', 'light rain', 'heavy rain')\nbike_data &lt;- cbind(bike_data, one_hot_encode_weathersit)\n\n# One hot for weekday\none_hot_encode_weekday &lt;- model.matrix(~ as.factor(weekday) - 1,data = bike_data)\none_hot_encode_weekday  &lt;- one_hot_encode_weekday[, -1] # Remove reference category\ncolnames(one_hot_encode_weekday) &lt;- c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')\nbike_data &lt;- cbind(bike_data, one_hot_encode_weekday)\n\n# One hot for season\none_hot_encode_season &lt;- model.matrix(~ as.factor(season) - 1,data = bike_data)\none_hot_encode_season  &lt;- one_hot_encode_season[, -1] # Remove reference category\ncolnames(one_hot_encode_season) &lt;- c('Spring', 'Summer', 'Fall')\nbike_data &lt;- cbind(bike_data, one_hot_encode_season)\n\n# Filter the dataset with the new dates\nbike_data_train &lt;- bike_data[bike_data$dteday &gt;= as.Date(\"2011-01-01\") & bike_data$dteday &lt;=  as.Date(\"2012-05-31\"), ]\nbike_data_test &lt;- bike_data[bike_data$dteday &gt;= as.Date(\"2012-06-01\") & bike_data$dteday &lt;=  as.Date(\"2012-12-31\"), ]\n\n# Response variables\ny_train &lt;- bike_data_train$log_cnt\ny_test &lt;- bike_data_test$log_cnt\n\n# Spline basis for 'hour' with 10 degrees of freedom for the training data\nspline_basis_train &lt;- ns(bike_data_train$hour, df = 10, intercept = FALSE)\n\n# Extract the knots\nknots &lt;- attr(spline_basis_train, \"knots\")\n\n# Intercept column to the design matrix\nintercept_train &lt;- rep(1, nrow(bike_data_train))\n\n# Combine the design matrix with the intercept, the spline basis and the other features \nx_train &lt;- cbind(intercept_train, spline_basis_train, bike_data_train[, c(\"yr\", \"holiday\", \"workingday\", \"temp\", \"atemp\", \"hum\", \"windspeed\",\"cloudy\", \"light rain\", \"heavy rain\",\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\",\"Spring\", \"Summer\", \"Fall\")])\n\n# Create a matrix for not having troubles when doing the Lasso model\nx_train &lt;- as.matrix(x_train)\n\n# Spline basis for the testing data using the same knots as in the training data\nspline_basis_test &lt;- ns(bike_data_test$hour, df = 10, knots = knots, intercept = FALSE)\n\n# Intercept column to the design matrix\nintercept_test &lt;- rep(1, nrow(bike_data_test))\n\n# Combine the design matrix with the intercept, the spline basis and the other features\nx_test &lt;- cbind(intercept_test, spline_basis_test, bike_data_test[, c(\"yr\", \"holiday\", \"workingday\", \"temp\", \"atemp\", \"hum\", \"windspeed\",\"cloudy\", \"light rain\", \"heavy rain\",\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\",\"Spring\", \"Summer\", \"Fall\")])\n\n# Matrix to use in the Lasso\nx_test &lt;- as.matrix(x_test)\n\n# To have a better looking Lasso Path\ncolnames(x_train)[2:11] &lt;- paste0(\"hour_spline_\", 1:10)\ncolnames(x_test)[2:11] &lt;- paste0(\"hour_spline_\", 1:10)\n\n# Fit a Lasso model (Lasso, alpha =1)\nlasso_model &lt;- cv.glmnet(x_train, y_train, alpha = 1, nfolds = 10, standardize = TRUE)\n\n# One-standard deviation rule to select lambda\noptimal_lambda &lt;- lasso_model$lambda.1se\n\n# Predict\ny_train_pred &lt;- predict(lasso_model, s = optimal_lambda, newx = x_train)\ny_test_pred &lt;- predict(lasso_model, s = optimal_lambda, newx = x_test)\n\n# Calculate RMSE's for training and testing sets\nrmse_train &lt;- sqrt(mean((y_train - y_train_pred)^2))\nrmse_test &lt;- sqrt(mean((y_test - y_test_pred)^2))\n\n# Print the RMSE's values\nprint(paste(\"The RMSE on Train Data using LASSO:\", rmse_train))\n\n[1] \"The RMSE on Train Data using LASSO: 0.650998201094293\"\n\nprint(paste(\"The RMSE on Test Data using LASSO:\", rmse_test))\n\n[1] \"The RMSE on Test Data using LASSO: 0.617649924980908\"\n\n\n\n\nðŸ‘¾ Problem 3.2\n\nWhich three features in Problem 3.1 seem to be the most important?\n\nTo explore which variables are most important we re-estimate the model in Problem 3.1 without cross-validation (using glmnet()). Then we apply the plot() function to the object returned by glmnet() using the arguments xvar=\"lambda and properly labeling the data, showing the Lasso path (Source link on how to use randomcoloR):\n\n# Lasso model without cross-validation (Lasso, alpha =1)\nlasso_model_full &lt;- glmnet(x_train, y_train, alpha = 1, standardize = TRUE)\n\n# Variable names\nvariable_names &lt;- colnames(x_train)\n\n# Package to generate various different random colors\nif (!require(randomcoloR)) install.packages(\"randomcoloR\")\nsuppressMessages(library(randomcoloR))\n\n# Margins\npar(mar = c(5, 4, 4, 8) + 0.1)\n\n# Different colors for each variable\ncolors &lt;- distinctColorPalette(length(variable_names))\n\n# Plot\nplot(lasso_model_full, xvar = \"lambda\", label = FALSE, cex.lab = 0.7, cex.axis = 0.8, col = colors)\ntitle(\"Lasso path\", line = 2.5)\nlegend(\"topright\", inset = c(-0.16, 0), legend = variable_names, col = colors, lty = 1, cex = 0.42, xpd = TRUE)\n\n\n\n\n\n\n\n\nAs shown in the Lasso path, the three most important variables are the temperature, the hour and the workingday.\n\n\nðŸ‘¾ Problem 3.3\n\nCarry out a residual analysis in Problem 3.1 for the training data. What can you say about the assumption of independent residuals? Repeat the same for the test data.\n\nThe following code calculate the residuals and plot the ACF for both, the training and the test data, so we can analyze the assumption of independent residuals:\n\n# Calculate residuals for the training data\nresiduals_train &lt;- y_train - y_train_pred\n\n# Plot ACF on training data\nacf(residuals_train, main = \"ACF of Training Residuals\")\n\n\n\n\n\n\n\n# Calculate residuals for the testing data\nresiduals_test &lt;- y_test - y_test_pred\n\n# Plot ACF on testing data\nacf(residuals_test, main = \"ACF of Testing Residuals\")\n\n\n\n\n\n\n\n\nThe ACF plots indicate that the assumption of independent residuals is violated for the training and testing data. The presence of significant autocorrelations suggests that the model may not have captured all relevant patterns in the data."
  },
  {
    "objectID": "index.html#regularised-time-series-regression-for-bike-rental-data",
    "href": "index.html#regularised-time-series-regression-for-bike-rental-data",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "4. Regularised time series regression for bike rental data",
    "text": "4. Regularised time series regression for bike rental data\n\nðŸ‘¾ Problem 4.1\n\nPlot a time series plot of the response in the original scale (i.e.Â counts and not log-counts) for the last week of the test data (last \\(24\\times 7\\) observations). In the same figure, plot a time series plot of the fitted values (in the original scale) from Problem 3.1. Comment on the fit.\n\nThe next code keeps the last week of data for the response variable as well as the Lasso fitted values in the original scale. Then it plots both in the same chart:\n\n# Last week of test data\nlast_week_data &lt;- tail(bike_data_test, 24 * 7)\n\n# To original scale\ncnt_pred &lt;- exp(y_test_pred)  \n\n# Fitted values for the last week\nlast_week_fitted &lt;- tail(cnt_pred, 24 * 7)\n\n# Time index for the last week\ntime_index &lt;- 1:(24 * 7)\n\n# Plot\nplot(time_index, last_week_data$cnt, type = 'l', col = 'cornflowerblue', lwd = 2,\n     xlab = 'Time (Hours)', ylab = 'Bike Rental Counts',\n     main = 'Bike Rental Counts: Actual vs Fitted for Last Week of Test Data',cex.main = 0.8)\n\n# Add the fitted values to the plot\nlines(time_index, last_week_fitted, col = 'lightcoral', lwd = 2, lty = 2)\n\n# Legend\nlegend(\"topleft\", legend = c(\"Actual\", \"Fitted\"), col = c(\"cornflowerblue\", \"lightcoral\"),\n       lty = c(1, 2), lwd = 2, cex = 0.5)\n\n\n\n\n\n\n\n\nAs shown in the graph, compared to the real data, the Lasso prediction overestimates the number of bike rentals in a vast amount of the last week data.\n\n\nðŸ‘¾ Problem 4.2\n\nAdd time series effects to your model by including some lagged values of the response as features. Use the first four hourly lags of log_cnt plus the 24th hourly lag as features, in addition to all other features in Problem 3.1. Fit the model using an L1 regularisation and find the optimal \\(\\lambda\\) by applying the one-standard deviation rule when cross-validating the training data using 10 folds. Compute the RMSE (using the optimal \\(\\lambda\\)) for the training and test data and compare to Problem 3.1. Are the residuals from this new model more adequate?\n\nThe next code add the first four hourly lags of log_cnt plus the 24th hourly lag as features using the library dplyr and store the new dataset in bike_nata_new. Then it fits the model using a Lasso regularisation and finds the optimal \\(\\lambda\\) by applying the one-standard deviation rule when cross-validating the training data using 10 folds. Moreover it computes the RMSE using that optimal lambda, for the training and test data and finally plots the ACF for both, training and test data:\n\n# Load packages\nif (!require(dplyr)) install.packages(\"dplyr\")\nsuppressMessages(library(dplyr))\n\n# Create lagged features using dplyr\nbike_data_new &lt;- bike_data %&gt;%\n  mutate(log_cnt_lag1 = lag(log_cnt, 1),\n         log_cnt_lag2 = lag(log_cnt, 2),\n         log_cnt_lag3 = lag(log_cnt, 3),\n         log_cnt_lag4 = lag(log_cnt, 4),\n         log_cnt_lag24 = lag(log_cnt, 24))\n\n# Remove NA's caused by lagging\nbike_data_new &lt;- na.omit(bike_data_new)\nbike_data_new$dteday &lt;- as.Date(bike_data_new$dteday, format = \"%Y-%m-%d\")\n\n# Split data into training and testing sets as the problem says\nbike_data_train_new &lt;- bike_data_new[bike_data_new$dteday &gt;= as.Date(\"2011-01-01\") & bike_data_new$dteday &lt;= as.Date(\"2012-05-31\"), ]\nbike_data_test_new &lt;- bike_data_new[bike_data_new$dteday &gt;= as.Date(\"2012-06-01\") & bike_data_new$dteday &lt;= as.Date(\"2012-12-31\"), ]\n\n# Response variables\ny_train &lt;- bike_data_train_new$log_cnt\ny_test &lt;- bike_data_test_new$log_cnt\n\n# Intercepts\nintercept_train_new &lt;- rep(1, nrow(bike_data_train_new))\nintercept_test_new &lt;- rep(1, nrow(bike_data_test_new))\n\n# Splines\n\nspline_basis_train_new &lt;- ns(bike_data_train_new$hour, df = 10, intercept = FALSE)\nknots_new &lt;- attr(spline_basis_train_new, \"knots\")\nspline_basis_test_new &lt;- ns(bike_data_test_new$hour, df = 10, knots = knots_new, intercept = FALSE)\n\n# Combine the design matrix with the intercept, the spline basis and the other features\nx_train_cb &lt;- cbind(intercept_train_new, spline_basis_train_new, bike_data_train_new[, c(\"yr\", \"holiday\", \"workingday\", \"temp\", \"atemp\", \"hum\", \"windspeed\", \"cloudy\", \"light rain\", \"heavy rain\",\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\",\"Spring\", \"Summer\", \"Fall\", \"log_cnt_lag1\", \"log_cnt_lag2\", \"log_cnt_lag3\", \"log_cnt_lag4\", \"log_cnt_lag24\")])\n\n# Create a matrix for not having troubles when doing the Lasso model\nx_train &lt;- as.matrix(x_train_cb)\n\n# Combine the design matrix with the intercept, the spline basis and the other features\nx_test_cb &lt;- cbind(intercept_test_new, spline_basis_test_new, bike_data_test_new[, c(\"yr\", \"holiday\", \"workingday\", \"temp\", \"atemp\", \"hum\", \"windspeed\", \"cloudy\", \"light rain\", \"heavy rain\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Spring\", \"Summer\", \"Fall\", \"log_cnt_lag1\", \"log_cnt_lag2\", \"log_cnt_lag3\", \"log_cnt_lag4\", \"log_cnt_lag24\")])\n\n# Create a matrix for not having troubles when doing the Lasso model\nx_test &lt;- as.matrix(x_test_cb)\n\n# Lasso model (alpha =1)\ncv_lasso &lt;- cv.glmnet(x_train, y_train, alpha = 1, nfolds = 10, standardize = TRUE)\n\n# Extract the optimal lambda using the one-standard deviation rule\noptimal_lambda &lt;- cv_lasso$lambda.1se\n\n# Predict\ny_train_pred &lt;- predict(cv_lasso, s = optimal_lambda, newx = x_train)\ny_test_pred &lt;- predict(cv_lasso, s = optimal_lambda, newx = x_test)\n\n# Calculate RMSE's\nrmse_train &lt;- sqrt(mean((y_train - y_train_pred)^2))\nrmse_test &lt;- sqrt(mean((y_test - y_test_pred)^2))\nprint(paste(\"The RMSE on Train Data:\", rmse_train))\n\n[1] \"The RMSE on Train Data: 0.42305594793291\"\n\nprint(paste(\"The RMSE on Test:\", rmse_test))\n\n[1] \"The RMSE on Test: 0.361916020668832\"\n\n# Residual's vector to plot\nresiduals_train &lt;- y_train - y_train_pred\nresiduals_test &lt;- y_test - y_test_pred\n\n# ACF of the residuals for the training data\nacf(residuals_train, main = \"ACF of Training Residuals (Model with Lagged Features)\", cex.main =0.6)\n\n\n\n\n\n\n\n# ACF of the residuals for the testing data\nacf(residuals_test, main = \"ACF of Testing Residuals (Model with Lagged Features)\",cex.main=0.6)\n\n\n\n\n\n\n\n\nThe RMSE for the model including some lagged values of the response as features is lower than the one that does not include them. This is due to the modelâ€™s enhanced ability to capture the temporal dependencies, seasonal patterns, and autocorrelations inherent in time series data, thatâ€™s also why, as shown in the ACF graphs, the residuals from this new model are more adequate than in 3.1.\n\n\nðŸ‘¾ Problem 4.3\n\nAdd the predictions from Problem 4.2 to the figure you created in Problem 4.1. Did the predictions improve by adding lags of the response variable?\n\nThe next code plots the response variable as well as the Lasso fitted values and the Lasso fitted values with lag in the original scale:\n\n# Time series plot\nplot(time_index, last_week_data$cnt, type = 'l', col = 'cornflowerblue', lwd = 2,\n     xlab = 'Time (Hours)', ylab = 'Bike Rental Counts',\n     main = 'Bike Rental Counts: Actual vs Fitted for Last Week of Test Data', cex.main = 0.8)\n\n# Add the fitted values to the plot\nlines(time_index, last_week_fitted, col = 'lightcoral', lwd = 2, lty = 2)\n\n\n# Predict the fitted values using the Lasso model with lagged features\nlog_cnt_pred_lags &lt;- y_test_pred \ncnt_pred_lags &lt;- exp(log_cnt_pred_lags) \n\n# Extract the fitted values for the last week from the lagged model\nlast_week_fitted_lags &lt;- tail(cnt_pred_lags, 24 * 7)\n\n# Add the new fitted values \nlines(time_index, last_week_fitted_lags, col = 'darkgreen', lwd = 2, lty = 3)\n\n# Legend\nlegend(\"topleft\", legend = c(\"Actual\", \"Fitted (Original)\", \"Fitted (Lagged Features)\"), col = c(\"cornflowerblue\", \"lightcoral\", \"darkgreen\"), lty = c(1, 2, 3), lwd= 1.5, cex=0.5)\n\n\n\n\n\n\n\n\nAs shown in the graph, adding lags of the response variable result in a better fit to the real data than when we add no lags."
  },
  {
    "objectID": "index.html#regression-trees-for-bike-rental-data",
    "href": "index.html#regression-trees-for-bike-rental-data",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "5. Regression trees for bike rental data",
    "text": "5. Regression trees for bike rental data\n\nðŸ‘¾ Problem 5.1\n\nUsing the training dataset from Problem 4.2 (that also includes lagged values of the response as features), fit a regression tree using the tree package. Experiment with the settings to see how changing them affects the results.\n\nThe next code uses the training dataset from problem 4.2 to fit a regression tree using the tree package. In commented lines, we can see which settings we can change in order to affect the results of the tree:\n\n# Load packages\nsuppressMessages(library(tree))\n\n# Design data\ntrain_ds &lt;- data.frame(y_train, x_train_cb)\ntest_ds &lt;- data.frame(y_test, x_test_cb)\ncolnames(train_ds)[1] &lt;- \"log_cnt\"\ncolnames(test_ds)[1] &lt;- \"log_cnt\"\nnames(train_ds)[names(train_ds) == \"intercept_train_new\"] &lt;- \"intercept\"\nnames(test_ds)[names(test_ds) == \"intercept_test_new\"] &lt;- \"intercept\"\n\n# Train model and fit\nlibrary(tree)\narbol &lt;- tree(log_cnt ~ ., data = train_ds)\n\n# These lines are used to change the parameters of the tree if I want to:\n#arbol &lt;- tree(log_cnt ~ ., data = train_ds, control = tree.control(nobs = nrow(train_ds), mincut = 2, minsize = 5, mindev = 0.01))\n\n#summary(arbol)\ny_hat_test &lt;- predict(arbol, newdata = test_ds)\n# Predict on the test data\ncnt_pred_tree &lt;- exp(y_hat_test)  # Transform back to original scale\n\n\n\nðŸ‘¾ Problem 5.2\n\nPlot the tree structure in Problem 5.1.\n\nThe next code plots the tree that we created in problem 5.1:\n\n# Plot the tree\nplot(arbol)\ntext(arbol, pretty = 0, cex = 0.38)\n\n# Add a title\ntitle(main = \"Regression Tree for Bike Rentals (with Lagged Features)\", cex.main = 0.50)\n\n\n\n\n\n\n\n\n\n\nðŸ‘¾ Problem 5.3\n\nAdd the predictions from Problem 5.1 to the figure you created in Problem 4.3. Comment on the fit of the regression tree compared to that of the semi-parametric spline approach.\n\nThe next code adds the prediction of the tree to the problemâ€™s 4.3 chart in order to compare which model fits best:\n\n# Create the time series plot\nplot(time_index, last_week_data$cnt, type = 'l', col = 'cornflowerblue', lwd = 2,\n     xlab = 'Time (Hours)', ylab = 'Bike Rental Counts', main = 'Bike Rental Counts: Actual vs Fitted for Last Week of Test Data', cex.main = 0.8)\n\n# Add the fitted values from the 4.1 model\nlines(time_index, last_week_fitted, col = 'lightcoral', lwd = 2, lty = 2)\n\n# Add the fitted values from the model with lagged features\nlines(time_index, last_week_fitted_lags, col = 'darkgreen', lwd = 2, lty = 3)\n\n# Add the tree model\nlast_week_fitted_tree &lt;- tail(cnt_pred_tree, 24 * 7)\nlines(time_index, last_week_fitted_tree, col = 'purple', lwd = 2, lty = 4)\n\n# Legend\nlegend(\"topleft\", legend = c(\"Actual\", \"Fitted (Original)\", \"Fitted (Lagged Features)\", \"Fitted (Regression Tree)\"),\n       col = c(\"cornflowerblue\", \"lightcoral\", \"darkgreen\", \"purple\"), lty = c(1, 2, 3, 4), lwd = 2, cex = 0.5)\n\n\n\n\n\n\n\n\nAs seen in the chart, the regression tree provides a decent fit but might be less smooth and somewhat more volatile compared to the semi-parametric spline approach with lagged features, which seems to better capture the underlying structure of the data."
  },
  {
    "objectID": "index.html#logistic-regression-for-classifying-spam-emails",
    "href": "index.html#logistic-regression-for-classifying-spam-emails",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "6. Logistic regression for classifying spam emails",
    "text": "6. Logistic regression for classifying spam emails\n\nðŸ‘¾ Problem 6.1\n\nReconstruct the confusion matrix for the test data without using the confusionMatrix() function.\n\nThis part of the code uses the confusionMatrix() function, in order to compare it with the one that we will make without using this package:\n\nrm(list=ls()) # Remove variables \ncat(\"\\014\") # Clean workspace\n\nload(file = '/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 1/spam_ham_emails.RData')\nSpam_ham_emails[, -1] &lt;- scale(Spam_ham_emails[, -1])\nSpam_ham_emails['spam'] &lt;- as.factor(Spam_ham_emails['spam'] == 1) # Changing from 1-&gt;TRUE, 0-&gt;FALSE\nlevels(Spam_ham_emails$spam) &lt;- c(\"not spam\", \"spam\")\n#head(Spam_ham_emails)\n#str(Spam_ham_emails)\n#cat(\"Percentage of spam:\", 100*mean(Spam_ham_emails$spam == \"spam\"))\nsuppressMessages(library(caret))\ntrain_obs &lt;- createDataPartition(y = Spam_ham_emails$spam, p = .75, list = FALSE)\ntrain &lt;- Spam_ham_emails[train_obs, ]\ntest &lt;- Spam_ham_emails[-train_obs, ]\n# Confirm both training and test are balanced with respect to spam emails\ncat(\"Percentage of training data consisting of spam emails:\", \n              100*mean(train$spam == \"spam\"))\n\nPercentage of training data consisting of spam emails: 39.40887\n\ncat(\"Percentage of test data consisting of spam emails:\", \n              100*mean(test$spam == \"spam\"))\n\nPercentage of test data consisting of spam emails: 39.3913\n\nglm_fit &lt;- glm(spam ~ ., family = binomial, data = train)\ny_prob_hat_test &lt;- predict(glm_fit, newdata = test, type = \"response\")\nthreshold &lt;- 0.5 # Predict spam if probability &gt; threshold\ny_hat_test &lt;- as.factor(y_prob_hat_test &gt; threshold)\nlevels(y_hat_test) &lt;- c(\"not spam\", \"spam\")\nconfusionMatrix(data = y_hat_test, test$spam, positive = \"spam\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction not spam spam\n  not spam      661   61\n  spam           36  392\n                                          \n               Accuracy : 0.9157          \n                 95% CI : (0.8981, 0.9311)\n    No Information Rate : 0.6061          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.8216          \n                                          \n Mcnemar's Test P-Value : 0.01482         \n                                          \n            Sensitivity : 0.8653          \n            Specificity : 0.9484          \n         Pos Pred Value : 0.9159          \n         Neg Pred Value : 0.9155          \n             Prevalence : 0.3939          \n         Detection Rate : 0.3409          \n   Detection Prevalence : 0.3722          \n      Balanced Accuracy : 0.9068          \n                                          \n       'Positive' Class : spam            \n                                          \n\n\nNow we create the confussion matrix by ourselves, calculating the true positives, false positives, true negatives and false negatives and putting them together on a matrix:\n\n# True positives (TP): Predicted spam and actual spam\nTP &lt;- sum(y_hat_test == \"spam\" & test$spam == \"spam\")\n\n# False positives (FP): Predicted spam but actual not spam\nFP &lt;- sum(y_hat_test == \"spam\" & test$spam == \"not spam\")\n\n# True negatives (TN): Predicted not spam and actual not spam\nTN &lt;- sum(y_hat_test == \"not spam\" & test$spam == \"not spam\")\n\n# False negatives (FN): Predicted not spam but actual spam\nFN &lt;- sum(y_hat_test == \"not spam\" & test$spam == \"spam\")\n\n# Construct the confusion matrix\nconfusion_matrix &lt;- matrix(c(TN, FN, FP, TP), nrow = 2, byrow = TRUE,dimnames = list('Reference' = c(\"not spam\", \"spam\"),'Prediction' = c(\"not spam\", \"spam\")))\n\n# Print the confusion matrix\nprint(confusion_matrix)\n\n          Prediction\nReference  not spam spam\n  not spam      661   61\n  spam           36  392\n\n\n\n\nðŸ‘¾ Problem 6.2\n\nCompute the accuracy, precision, sensitivity (recall), and specificity without using the confusionMatrix() function. Explain these four concepts in the context of the spam filter.\n\nThe next code uses the true positives, false positives, true negatives and false negatives obtained in problem 6.1 to calculate the measures asked in problem 6.2:\n\n# Calculate the metrics\naccuracy &lt;- (TP + TN) / (TP + TN + FP + FN)\nprecision &lt;- TP / (TP + FP)\nsensitivity &lt;- TP / (TP + FN)\nspecificity &lt;- TN / (TN + FP)\n\n# Print the metrics\nprint(paste(\"Accuracy:\", accuracy))\n\n[1] \"Accuracy: 0.915652173913043\"\n\nprint(paste(\"Precision:\", precision))\n\n[1] \"Precision: 0.91588785046729\"\n\nprint(paste(\"Sensitivity:\", sensitivity))\n\n[1] \"Sensitivity: 0.865342163355408\"\n\nprint(paste(\"Specificity:\", specificity))\n\n[1] \"Specificity: 0.948350071736011\"\n\n\nAccuracy tells us how often the spam filter correctly classifies emails, whether as spam or not spam, in this case the accuracy is 90%, so the filter is a good classifier.\n\\[\\begin{align*}\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n\\end{align*}\\]\nPrecision measures how reliable the spam predictions are. Here, 88% of the emails classified as spam are indeed spam, with fewer false alarms (not spam emails mistakenly marked as spam).\n\\[\\begin{align*}\n\\text{Precision} &= \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \\\\\n\\end{align*}\\]\nSensitivity tells us how good the filter is at detecting spam. Here, the filter catches 85% of the spam emails.\n\\[\\begin{align*}\n\\text{Sensitivity} &= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\\\\n\\end{align*}\\]\nSpecificity measures how good the filter is at correctly identifying not spam emails. Here, the filter does not mark the 93% of legitimate emails as spam.\n\\[\\begin{align*}\n\\text{Specificity} &= \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n\\end{align*}\\]\n\n\nðŸ‘¾ Problem 6.3\n\nCompute the ROC curve using the pROC() package. Explain in detail what the ROC curve shows.\n\nThe next code plots the ROC curve, that is, the true positive rate (TPR) against the false positive rate (FPR) at various thresholds:\n\n# Package\nif (!require(pROC)) install.packages(\"pROC\")\nsuppressMessages(library(pROC))\n\n# Calculate the ROC curve\nroc_curve &lt;- roc(test$spam, y_prob_hat_test)\npar(pty = 's')\n# Plot\nplot(roc_curve, main = \"ROC Curve for Spam Filter\", col = \"cornflowerblue\", lwd = 2, print.auc = TRUE)\n\n\n\n\n\n\n\n\nThe ROC plots the true positive rate (TPR) against the false positive rate (FPR) at various thresholds. It is helpful to understand the trade-offs between both of them. By examining the ROC curve, one can choose a threshold that balances sensitivity and specificity according to the specific needs of the modeler, in this case, as we want to minimize false negatives (missing spam e-mails), we can choose a threshold that gives a higher sensitivity."
  },
  {
    "objectID": "index.html#decision-trees-for-classifying-spam-emails",
    "href": "index.html#decision-trees-for-classifying-spam-emails",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "7. Decision trees for classifying spam emails",
    "text": "7. Decision trees for classifying spam emails\n\nðŸ‘¾ Problem 7.1\n\nUsing the training dataset from Problem 6, fit a decision tree (classification tree) using the tree package with the default settings. How does this classifier perform on the test dataset compared to the logistic classifier in Problem 6?\n\nThe next code fits a decision tree using the tree package with the default settings using the training dataset from problem 6 and calculates its accuracy, precision, sensitivity and specificity in order to compare them against the logistic regression classifier:\n\n# Decision tree model using the training data\ntree_fit &lt;- tree(spam ~ ., data = train)\n\n# Print the summary\n#summary(tree_fit)\n\n# Predict on the test dataset, we use class as a classification problem parameter identifier\ntree_pred &lt;- predict(tree_fit, newdata = test, type = \"class\")\n\n# Confusion matrix for the decision tree\ntree_confusion_matrix &lt;- table(Predicted = tree_pred, Actual = test$spam)\nprint(tree_confusion_matrix)\n\n          Actual\nPredicted  not spam spam\n  not spam      664   78\n  spam           33  375\n\n# Extract confusion matrix components\nTN_tree &lt;- tree_confusion_matrix[1, 1]\nFP_tree &lt;- tree_confusion_matrix[1, 2]\nFN_tree &lt;- tree_confusion_matrix[2, 1]\nTP_tree &lt;- tree_confusion_matrix[2, 2]\n\n# Calculate performance metrics\naccuracy_tree &lt;- (TP_tree + TN_tree) / sum(tree_confusion_matrix)\nprecision_tree &lt;- TP_tree / (TP_tree + FP_tree)\nsensitivity_tree &lt;- TP_tree / (TP_tree + FN_tree)\nspecificity_tree &lt;- TN_tree / (TN_tree + FP_tree)\n\n# Print the metrics\nprint(paste(\"Accuracy:\", accuracy_tree))\n\n[1] \"Accuracy: 0.903478260869565\"\n\nprint(paste(\"Precision:\", precision_tree))\n\n[1] \"Precision: 0.827814569536424\"\n\nprint(paste(\"Sensitivity:\", sensitivity_tree))\n\n[1] \"Sensitivity: 0.919117647058823\"\n\nprint(paste(\"Specificity:\", specificity_tree))\n\n[1] \"Specificity: 0.894878706199461\"\n\n\nCompared to the logistic regression classifier, the only metric that slightly improves when using a tree, is the sensitivity, which means that the tree is, in overall, better at detecting true positives than the logistic regression classifier.\n\n\nðŸ‘¾ Problem 7.2\n\nPlot the tree structure in Problem 7.1.\n\nThe next code plots the tree structure of problem 7.1:\n\n# Adjust margins using mai (in inches)\npar(mai = c(0.1, 0.1, 0.1, 0.1))  # Bottom, left, top, right\n\n# Plot the decision tree with adjusted margins\nplot(tree_fit, uniform = TRUE, margin = 0.1, cex.main = 0.8, branch = 2)\ntitle(main = \"Decision Tree for Spam Classification\", cex.main = 0.8)\n\n# Labels\ntext(tree_fit, use.n = TRUE, all = TRUE, cex = 0.37, pretty = 0)"
  },
  {
    "objectID": "index.html#k-nearest-neighbour-for-classifying-spam-emails",
    "href": "index.html#k-nearest-neighbour-for-classifying-spam-emails",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "8. k-nearest neighbour for classifying spam emails",
    "text": "8. k-nearest neighbour for classifying spam emails\n\nðŸ‘¾ Problem 8.1\n\nWithout using any package, fit a k-nearest neighbour classifier to the training dataset from Problem 6. Choose the value of \\(k\\) that minimises some suitable error function for the test data.\n\nThe next code fits a k-nearest neighbour classifier to the training dataset from problem 6, the value of \\(k\\) is chosen among 15 different \\(k\\)â€™s minimising the MSE as an error function:\n\n# Euclidean distance function\neuclidean_distance = function(x1, x2)\n  #  We check that they have the same number of observation\n  {if(length(x1) == length(x2)){\n    sqrt(sum((x1-x2)^2))  \n  } else{\n    stop('Vectors must be of the same length')\n  }}\n\n# k-NN function\nknn &lt;- function(train_data, train_labels, test_data, k) \n  {predictions &lt;- rep(NA, nrow(test_data))\n  for (i in 1:nrow(test_data)) {\n    distances &lt;- apply(train_data, 1, euclidean_distance, x2 = test_data[i, ])\n    neighbor_indices &lt;- order(distances)[1:k]\n    neighbor_labels &lt;- train_labels[neighbor_indices]\n    predictions[i] &lt;- ifelse(mean(neighbor_labels == \"spam\") &gt; 0.5, \"spam\", \"not spam\")} # If more than 50% of the neighbors is labeled as spam, then label the prediction as spam\n  return(factor(predictions, levels = levels(train_labels)))}\n\n# Standardizing the predictors\ntrain_data &lt;- scale(train[, -1])\ntest_data &lt;- scale(test[, -1])\ntrain_labels &lt;- train$spam\ntest_labels &lt;- test$spam\n\n# Initialize variables\nk_values &lt;- 1:15 # Range of k values to try\nmse_values &lt;- numeric(length(k_values)) \n\n# Loop\nfor (i in seq_along(k_values)) \n  {k &lt;- k_values[i]\n  \n  # Predict using k-NN\n  predictions &lt;- knn(train_data, train_labels, test_data, k)\n  \n  # Convert predictions to numeric for MSE calculation\n  predictions_numeric &lt;- as.numeric(predictions) - 1\n  \n  # Calculate MSE\n  mse_values[i] &lt;- mean((predictions_numeric - (as.numeric(test_labels) - 1))^2)}\n\n# Find the optimal k that minimizes MSE\noptimal_k &lt;- k_values[which.min(mse_values)]\nprint(paste(\"The k that minimises the MSE is:\", optimal_k))\n\n[1] \"The k that minimises the MSE is: 3\"\n\n# Evaluate the model with optimal k\nfinal_predictions &lt;- knn(train_data, train_labels, test_data, k = optimal_k)\n\n\n\nðŸ‘¾ Problem 8.2\n\nHow does the classifier in Problem 8.1 perform on the test dataset compared to the logistic classifier in Problem 6 and the decision tree classifier in Problem 7?\n\nThe next code calculates and prints the classifier evaluation metrics in order to compare it against problems 6.1 and 7.1:\n\n# Confusion matrix\nTP_knn &lt;- sum(final_predictions == \"spam\" & test_labels == \"spam\")\nTN_knn &lt;- sum(final_predictions == \"not spam\" & test_labels == \"not spam\")\nFP_knn &lt;- sum(final_predictions == \"spam\" & test_labels == \"not spam\")\nFN_knn &lt;- sum(final_predictions == \"not spam\" & test_labels == \"spam\")\n\n# Calculate performance metrics\naccuracy_knn &lt;- (TP_knn + TN_knn) / (TP_knn + TN_knn + FP_knn + FN_knn)\nprecision_knn &lt;- TP_knn / (TP_knn + FP_knn)\nsensitivity_knn &lt;- TP_knn / (TP_knn + FN_knn)\nspecificity_knn &lt;- TN_knn / (TN_knn + FP_knn)\n\n# Print the metrics\nprint(paste(\"Accuracy:\", accuracy_knn))\n\n[1] \"Accuracy: 0.920869565217391\"\n\nprint(paste(\"Precision:\", precision_knn))\n\n[1] \"Precision: 0.907657657657658\"\n\nprint(paste(\"Sensitivity:\", sensitivity_knn))\n\n[1] \"Sensitivity: 0.88962472406181\"\n\nprint(paste(\"Specificity:\", specificity_knn))\n\n[1] \"Specificity: 0.941176470588235\"\n\n\nThe k-nn classifier has a better performance in all the metrics compared to the logistic regression classifier meaning that, in overall, the k-nn is a better classifier. Compared to the tree, the k-nn classifier improves all the metrics except for the sensitivity, which means that, in overall, the tree is better at detecting true positives than the k-nn classifier."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In this ML session we treat topics such as polynomial and spline regression, regularisation, cross-validation, regression and decision trees, and classification methods. Here we consider modelling the number of rides each hour for a bike rental company."
  }
]